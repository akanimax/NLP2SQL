{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I demonstrate how to create multi-layered RNN using the tensorflow's dynamic_rnn module.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Technology used: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset used for this notebook is the Movie-review-Sentiment-Analysis from kaggle\n",
    "link -> https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with the usual utility cells and then proceed with preliminary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            # packages used for processing: \n",
    "import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# the boss of frameworks\n",
    "import tensorflow as tf\n",
    "\n",
    "# for dataset building:\n",
    "import collections\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Models\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data/sentiment_analysis_kaggle/\" # the data path\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(data_path, \"train.tsv\"),\n",
    "    \"test\": os.path.join(data_path, \"test.tsv\")\n",
    "}\n",
    "\n",
    "base_model_path = '../Models'\n",
    "\n",
    "model_name = 'Sentiment_Analysis_Model_1'\n",
    "\n",
    "tensorboard_log_dir = os.path.join(base_model_path, model_name)\n",
    "\n",
    "model_save_path = tensorboard_log_dir\n",
    "\n",
    "model_save_filename = os.path.join(model_save_path, model_name)\n",
    "\n",
    "plug_and_play_data_file_path = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "submission_file = os.path.join(data_path, \"submission\" + model_name + \".csv\")\n",
    "\n",
    "# constants:\n",
    "vocabulary_size = 15000\n",
    "PAD = 0\n",
    "hidden_cell_state_size = 512 # size of the LSTM cell_state\n",
    "embedding_size = 128\n",
    "num_classes = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "no_of_epochs = 3\n",
    "check_point_factor = 2 # save after seeing 500 minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There doesn't seem to be any obvious package to load this tsv file. So, I am writing a \n",
    "# function myself to extract the training data from the train.tsv file\n",
    "\n",
    "def get_train_data(file_path, feed_back = True, feed_back_factor = 100):\n",
    "    '''\n",
    "        function to load the data from the given file_path and generate a proper data_structure for it.\n",
    "        @param\n",
    "        file_path => the path where the file is located\n",
    "        @return => a list of dictionaries of the formatted data\n",
    "    '''\n",
    "    # open the file and start reading it line by line\n",
    "    with open(file_path, \"r\") as data_file:\n",
    "        heading = data_file.readline()\n",
    "        dict_keys = heading.lower().split() # split the heading to generate the keys for the json dicts\n",
    "        \n",
    "        data = [] # initialize the data to an empty list\n",
    "        # parse the remaining lines and convert them to structured dictionaries\n",
    "        count = 1 # for feedback purposes\n",
    "        for line in data_file:\n",
    "            # split the line at '\\t'\n",
    "            vals = line.strip().split(\"\\t\")\n",
    "            element_dict = {} # initialize to empty dictionary\n",
    "            \n",
    "            # now, put the parsed values in the dict\n",
    "            element_dict[dict_keys[0]] = int(vals[0])\n",
    "            element_dict[dict_keys[1]] = int(vals[1])\n",
    "            element_dict[dict_keys[2]] = vals[2]\n",
    "            element_dict[dict_keys[3]] = int(vals[3])\n",
    "            \n",
    "            # append this newly formed dictionary to the data list\n",
    "            data.append(element_dict)\n",
    "            \n",
    "            if(feed_back and count % feed_back_factor == 0):\n",
    "                print \"Currently processing line number: \" + str(count)\n",
    "                print \"data extracted from this line: \" + str(element_dict) + \"\\n\\n\"\n",
    "            \n",
    "            count += 1 # increment the counter\n",
    "            \n",
    "    # return the so created data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = get_train_data(data_files[\"train\"], feed_back=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phrase': 'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',\n",
       " 'phraseid': 1,\n",
       " 'sentenceid': 1,\n",
       " 'sentiment': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data examples to train on: 156060\n"
     ]
    }
   ],
   "source": [
    "print \"Total data examples to train on: \" + str(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in the dataset: 1124157\n"
     ]
    }
   ],
   "source": [
    "# build a list of words for developing a vocabulary:\n",
    "words = []\n",
    "for elem in data:\n",
    "    words += elem[\"phrase\"].lower().split()\n",
    "    \n",
    "print \"total words in the dataset: \" + str(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the dataset: 16531\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set(words))\n",
    "print \"Unique words in the dataset: \" + str(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the dataset: <br>\n",
    "using the helper from https://github.com/akanimax/machine-learning-helpers/blob/master/text/create_vocabulary_for_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections # only dependency for this function\n",
    "\n",
    "'''\n",
    "    Note: this function assumes the input as a list of words in a meaningful sequence. This function can be easily modified for handling list of sequences as input for special cases of seq2seq models.\n",
    "'''\n",
    "\n",
    "def build_vocabulary(words, n_words):\n",
    "    \"\"\"\n",
    "        Process raw inputs into a dataset.\n",
    "        @param\n",
    "        words => the list of all the words in the dataset\n",
    "        @return => word_count, words_dictionary, words_reverse_dictionary\n",
    "    \"\"\"\n",
    "    count = [['BNK', 0], ['UNK', -1]] # start with this list.\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 2)) # this is inplace. i.e. has a side effect\n",
    "\n",
    "    dictionary = dict() # initialize the dictionary to empty one\n",
    "    # fill this dictionary with the most frequent words\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "  \n",
    "    # construct the reverse dictionary for the original dictionary\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    # return all the relevant stuff\t\n",
    "    return count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the count, dictionary and the reverse_dictionary for the given dataset. \n",
    "count, dictionary, reverse_dictionary = build_vocabulary(words, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use the vocabulary to transform the data into sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_dicts(in_dicts, dictionary):\n",
    "    '''\n",
    "        Function to transform the input lists of words into numerical sequences using the dictionary\n",
    "        @param\n",
    "        in_dicts => the list of input data dictionaries\n",
    "        dictionary => the word mapping from every word to an integer\n",
    "        @return => input_sequences, sentiment class of those sequences\n",
    "    '''\n",
    "    input_sequences = []; labels = [] # initialize them to empty ones\n",
    "    \n",
    "    # iterate over every data element dictionary in the data\n",
    "    for elem_dict in data:\n",
    "        seq = elem_dict[\"phrase\"].lower().split()\n",
    "        label = elem_dict[\"sentiment\"]\n",
    "        \n",
    "        # transform the seq using the dictionary\n",
    "        seq = map(lambda x: dictionary[x] if x in dictionary else dictionary['UNK'], seq)\n",
    "        \n",
    "        # print for debugging purposes\n",
    "        # print \"Mapped Sequence: \" + str(seq)\n",
    "        \n",
    "        input_sequences.append(seq); labels.append(label)\n",
    "        \n",
    "    # return the so formed input_sequences and the labels\n",
    "    return input_sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequences, sentiment_labels = transform_dicts(data, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 156060)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(input_sequences) == len(sentiment_labels), \"Data has been corrupted. Seqs and labels length not equal\"\n",
    "len(input_sequences), len(sentiment_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, We have to only once shuffle and save that data permanently, so that the distributions of the train, test and dev set don't change\n",
    "\n",
    "I have done this process and saved the data in the plug_and_play.pickle file. Simply load that file to get the shuffled and split data. You can skip directly to next section, although, there won't be any harm if you do them again. The notebook will only use the originally pickled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to perform synchronous random shuffling of the training data\n",
    "def synch_random_shuffle_non_np(X, Y):\n",
    "    '''\n",
    "        ** This function takes in the parameters that are non numpy compliant dtypes such as list, tuple, etc.\n",
    "        Although this function works on numpy arrays as well, this is not as performant enough\n",
    "        @param\n",
    "        X, Y => The data to be shuffled\n",
    "        @return => The shuffled data\n",
    "    '''\n",
    "    combined = zip(X, Y)\n",
    "    \n",
    "    # shuffle the combined list in place\n",
    "    np.random.shuffle(combined)\n",
    "    \n",
    "    # extract the data back from the combined list\n",
    "    X[:], Y[:] = zip(*combined)\n",
    "    \n",
    "    # return the shuffled data:\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequences, sentiment_labels = synch_random_shuffle_non_np(input_sequences, sentiment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split the data into train - dev sets:\n",
    "def split_train_dev(X, Y, train_percentage):\n",
    "    '''\n",
    "        function to split the given data into two small datasets (train - dev)\n",
    "        @param\n",
    "        X, Y => the data to be split\n",
    "        (** Make sure the train dimension is the first one)\n",
    "        train_percentage => the percentage which should be in the training set. \n",
    "        (**this should be in 100% not decimal)\n",
    "        \n",
    "        @return => train_X, train_Y, test_X, test_Y\n",
    "    '''\n",
    "    m_examples = len(X)\n",
    "    assert train_percentage < 100, \"Train percentage cannot be greater than 100! NOOB!\"\n",
    "    partition_point = int((m_examples * (float(train_percentage) / 100)) + 0.5) # 0.5 is added for rounding\n",
    "    \n",
    "    # construct the train_X, train_Y, test_X, test_Y sets:\n",
    "    train_X = X[: partition_point]; train_Y = Y[: partition_point]\n",
    "    test_X  = X[partition_point: ]; test_Y  = Y[partition_point: ]\n",
    "    \n",
    "    assert len(train_X) + len(test_X) == m_examples, \"Something wrong in X splitting\"\n",
    "    assert len(train_Y) + len(test_Y) == m_examples, \"Something wrong in Y splitting\"\n",
    "    \n",
    "    # return the constructed sets\n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = split_train_dev(input_sequences, sentiment_labels, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148257 148257 7803 7803\n"
     ]
    }
   ],
   "source": [
    "print len(train_X), len(train_Y), len(test_X), len(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to pickle an object\n",
    "def pickleIt(obj, save_path):\n",
    "    '''\n",
    "        function to pickle the given object.\n",
    "        @param\n",
    "        obj => the python object to be pickled\n",
    "        save_path => the path where the pickled file is to be saved\n",
    "        @return => nothing (the pickle file gets saved at the given location)\n",
    "    '''\n",
    "    if(not os.path.isfile(save_path)):\n",
    "        with open(save_path, 'wb') as dumping:\n",
    "            pickle.dump(obj, dumping, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print \"The file has been pickled at: \" + save_path\n",
    "\n",
    "    else:\n",
    "        print \"The pickle file already exists: \" + save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the saving dictionary:\n",
    "data_dict = {\n",
    "    'train_X': train_X,\n",
    "    'train_Y': train_Y,\n",
    "    'test_X' : test_X,\n",
    "    'test_Y' : test_Y,\n",
    "    'labels' : {\n",
    "        0: 'negative',\n",
    "        1: 'somewhat negative',\n",
    "        2: 'neutral',\n",
    "        3: 'somewhat positive',\n",
    "        4: 'positive'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pickle file already exists: ../Data/sentiment_analysis_kaggle/plug_and_play.pickle\n"
     ]
    }
   ],
   "source": [
    "# pickle the data\n",
    "pickleIt(data_dict, plug_and_play_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use the function from the helpers repo to unpickle the data:\n",
    "\n",
    "link to function -> https://github.com/akanimax/machine-learning-helpers/blob/master/pickling_unpickling/pickling_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unpickle the given file and load the obj back into the python environment\n",
    "def unPickleIt(pickle_path): # might throw the file not found exception\n",
    "    '''\n",
    "        function to unpickle the object from the given path\n",
    "        @param\n",
    "        pickle_path => the path where the pickle file is located\n",
    "        @return => the object extracted from the saved path\n",
    "    '''\n",
    "\n",
    "    with open(pickle_path, 'rb') as dumped_pickle:\n",
    "        obj = pickle.load(dumped_pickle)\n",
    "\n",
    "    return obj # return the unpickled object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = unPickleIt(plug_and_play_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = data_dict['train_X']\n",
    "train_Y = data_dict['train_Y']\n",
    "test_X = data_dict['test_X']\n",
    "test_Y = data_dict['test_Y']\n",
    "labels = data_dict['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative',\n",
       " 1: 'somewhat negative',\n",
       " 2: 'neutral',\n",
       " 3: 'somewhat positive',\n",
       " 4: 'positive'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a function to pad the batch of sequences into a fixed length (by padding) numpy array\n",
    "def pad(seqs):\n",
    "    '''\n",
    "        function to convert the list of seqs into a batch tensor (padding the batch to a nice length)\n",
    "        @param\n",
    "        seqs => the list of variable length scalar sequences\n",
    "        @return => The batch tensor converted using the seqs\n",
    "    '''\n",
    "    \n",
    "    lengths = map(lambda x: len(x), seqs) # extract the lengths of all the sequences in the batch\n",
    "    max_length = max(lengths) # calculate the max of those lengths\n",
    "    \n",
    "    converted_seqs = [] # initialize it to empty list\n",
    "    # for every sequence, pad it upto the length of max_length\n",
    "    for seq in seqs: \n",
    "        while(len(seq) != max_length):\n",
    "            seq = seq + [PAD]\n",
    "        # now append this list to the converted_seqs\n",
    "        converted_seqs.append(seq)\n",
    "        \n",
    "    # return the numpy array corredponding to the converted_seqs\n",
    "    return np.array(converted_seqs).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# So, now the setup is done. Let's move on to the actual dynamic_rnn_building\n",
    "## I will use the InteractiveSession() to work with this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything down below goes wrong, try executing from this reset point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create placeholders for input_sequences and input_labels\n",
    "with tf.variable_scope(\"Inputs\"):\n",
    "    tf_input_seqs = tf.placeholder(tf.int32, shape=(None, None), name='input_sequences')\n",
    "    tf_senti_labs = tf.placeholder(tf.int32, shape=(None), name='sentiment_labels')\n",
    "    one_hot_encoded_senti_labs = tf.one_hot(tf_senti_labs, depth=num_classes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Embedding\"):\n",
    "    embedding_matrix = tf.get_variable(\"embedding_matrix\", \n",
    "                                shape=(vocabulary_size, embedding_size), initializer=tf.random_uniform_initializer())\n",
    "\n",
    "    # obtain the embedded version of input\n",
    "    embedded_tf_input_seqs = tf.nn.embedding_lookup(embedding_matrix, tf_input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Embedding/embedding_lookup:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tf_input_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create single layer of dynamic_rnn:\n",
    "with tf.variable_scope(\"RNN_layer_1\"):\n",
    "    lay1_outputs, lay1_states = tf.nn.dynamic_rnn (\n",
    "                                    tf.nn.rnn_cell.LSTMCell(hidden_cell_state_size, use_peepholes = True),\n",
    "                                    embedded_tf_input_seqs,\n",
    "                                    time_major = True, # the batch size is along the columns\n",
    "                                    dtype = tf.float32 # we have to specify this since there is no initial state\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'RNN_layer_1/rnn/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 512) dtype=float32>, LSTMStateTuple(c=<tf.Tensor 'RNN_layer_1/rnn/while/Exit_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'RNN_layer_1/rnn/while/Exit_3:0' shape=(?, 512) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "# print the tensor information of lay1_outputs and lay1_states\n",
    "print (lay1_outputs, lay1_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to build the second layer of the dynamic_rnn:\n",
    "# for using a different LSTM cell for this layer, the variable scope of this layer needs to be different \n",
    "# from the earlier layer\n",
    "with tf.variable_scope(\"RNN_layer_2\"):\n",
    "    lay2_outputs, lay2_states = tf.nn.dynamic_rnn (\n",
    "                                    tf.nn.rnn_cell.LSTMCell(hidden_cell_state_size, use_peepholes=True),\n",
    "                                    lay1_outputs, # output of the previous layer is input to this layer\n",
    "                                    time_major = True,\n",
    "                                    initial_state = lay1_states\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'RNN_layer_2/rnn/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 512) dtype=float32>, LSTMStateTuple(c=<tf.Tensor 'RNN_layer_2/rnn/while/Exit_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'RNN_layer_2/rnn/while/Exit_3:0' shape=(?, 512) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "print (lay2_outputs, lay2_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to build the last layer of the dynamic lstm network\n",
    "with tf.variable_scope(\"RNN_layer3\"):\n",
    "    _, lay3_states = tf.nn.dynamic_rnn (\n",
    "                        tf.nn.rnn_cell.LSTMCell(hidden_cell_state_size, use_peepholes=True),\n",
    "                        lay2_outputs, # output of the previous layer is again the input of this layer\n",
    "                        time_major = True,\n",
    "                        initial_state = lay2_states\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"RNN_layer3/rnn/while/Exit_3:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Now, check what the last layers output state is:\n",
    "pre_projected_logits = lay3_states.h\n",
    "print pre_projected_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will have to project this in order to obtain the logits from the lay3_states.h tensor\n",
    "with tf.variable_scope(\"Final_projection\"):\n",
    "    parameters = {\n",
    "        'weights': tf.get_variable('Final_weights', shape=(hidden_cell_state_size, num_classes), \n",
    "                                   initializer=tf.random_normal_initializer()),\n",
    "        'biases' : tf.get_variable('Final_biases', shape=(1, num_classes), initializer=tf.zeros_initializer())\n",
    "    }\n",
    "\n",
    "    # obtain the last layer activations:\n",
    "    projected_logits = tf.matmul(pre_projected_logits, parameters['weights']) + parameters['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Final_projection/add:0' shape=(?, 5) dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Loss\"):\n",
    "    # calculate the loss between the projected_logits and the one_hot encoded logits\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=projected_logits, \n",
    "                                                                    labels=one_hot_encoded_senti_labs))\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "with tf.variable_scope(\"Trainer\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    # manually take the softmax of the projected logits to obtain the predictions:\n",
    "    predictions = tf.nn.softmax(projected_logits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errands Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the init op\n",
    "with tf.variable_scope(\"INIT\"):\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the merged summaries op\n",
    "with tf.variable_scope(\"ALL_SUMMARIES\"):\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize this graph in Tensorboard to make sure everything is properly wired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the init op\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the tensorboard_visualizer:\n",
    "tensorboard_wirter = tf.summary.FileWriter(logdir=tensorboard_log_dir, graph=sess.graph, filename_suffix='.bot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The graph looks cool! lets move forward with the actual training of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_train_examples = len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create minibatches of the train_X and train_Y\n",
    "minibatches = [] # initialize to empty list\n",
    "index = 0 # initialize to zero\n",
    "for _ in range(int(np.ceil(float(total_train_examples) / batch_size))):\n",
    "    start = index; end = start + batch_size\n",
    "    mini_batch = (train_X[start: end], train_Y[start: end])\n",
    "    \n",
    "    # add the minibatch to the minibatches\n",
    "    minibatches.append(mini_batch)\n",
    "    \n",
    "    # update the index \n",
    "    index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 33, 64, 64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minibatches[-1][0]), len(minibatches[-1][1]), len(minibatches[-2][0]), len(minibatches[-2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start the training iterations\n",
    "for epoch in range(no_of_epochs):\n",
    "    print \"Currently doing: epoch \" + str(epoch + 1)\n",
    "    print \"=======================================================================================================\"\n",
    "    \n",
    "    # iterate over every minibatch\n",
    "    for iteration in range(len(minibatches)):\n",
    "        (mini_train_X, mini_train_Y) = minibatches[iteration]\n",
    "        \n",
    "        # compute the train_step.\n",
    "        sess.run(train_step, feed_dict={tf_input_seqs: pad(mini_train_X), tf_senti_labs: mini_train_Y})\n",
    "        \n",
    "        # if it is checkpoint factor:\n",
    "        if((iteration + 1) % check_point_factor == 0):\n",
    "            # compute the cost and the summary for cost\n",
    "            summaries, cost = sess.run((all_summaries, loss), feed_dict={tf_input_seqs: pad(mini_train_X), tf_senti_labs: mini_train_Y})\n",
    "            \n",
    "            # print a small message for feedback\n",
    "            print str(iteration + 1) + \".) Current Loss: \" + str(cost)\n",
    "            \n",
    "            # add the summary\n",
    "            tensorboard_wirter.add_summary(summaries, global_step=(iteration + 1))\n",
    "    \n",
    "    print \"=======================================================================================================\\n\\n\"\n",
    "    \n",
    "    # save the model after every epoch\n",
    "    saver.save(sess, model_save_filename, global_step=(epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to evaluate the model to calculate the accuracy on the test_X, test_Y set (technically, the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7803 7803\n"
     ]
    }
   ],
   "source": [
    "print len(test_X), len(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Sentiment_Analysis_Model_1/Sentiment_Analysis_Model_1-2\n"
     ]
    }
   ],
   "source": [
    "# use the trained model for obtaining predictions:\n",
    "assert os.path.isfile(os.path.join(model_save_path, \"checkpoint\")), \"Model doesn't exist\"\n",
    "\n",
    "saver.restore(sess, tf.train.latest_checkpoint(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' !!! WARNING WARNING WARNING !!! ''' ''' Heavy computation cell'''\n",
    "\n",
    "# calculate the predictions for the test set\n",
    "preds, labs = sess.run((predictions, one_hot_encoded_senti_labs), \n",
    "                           feed_dict={tf_input_seqs: pad(test_X), tf_senti_labs: test_Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6ccc15b5b627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print the shapes of the preds and labs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# print the shapes of the preds and labs\n",
    "print preds.shape, labs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool they are compatible.\n",
    "print preds[0], labs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the accuracy of this model:\n",
    "correct = np.argmax(preds, axis=1) == np.argmax(labs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.sum(correct.astype(np.int32))\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = (float(correct) / m_test_examples) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the obtained accuracy for this classifier:\n",
    "print accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok! Not bad! on the dev set.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Lets generate the submission.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test.tsv file using the utility function\n",
    "import pandas as pd\n",
    "\n",
    "to_be_predicted_data = pd.read_csv(data_files['test'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156066</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>156067</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>156068</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156069</td>\n",
       "      <td>8545</td>\n",
       "      <td>pleasing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156070</td>\n",
       "      <td>8545</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine\n",
       "5    156066        8545                        intermittently pleasing but\n",
       "6    156067        8545                            intermittently pleasing\n",
       "7    156068        8545                                     intermittently\n",
       "8    156069        8545                                           pleasing\n",
       "9    156070        8545                                                but"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_predicted_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([156061, 156062, 156063, ..., 222350, 222351, 222352])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.array(to_be_predicted_data.PhraseId)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intermittently pleasing but mostly routine effort'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the phrases data:\n",
    "phrase_data = list(to_be_predicted_data.Phrase)\n",
    "phrase_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the phrase data using the dictionary that we have\n",
    "feed_data = [] # initialize to empty list\n",
    "for phrase in phrase_data:\n",
    "    datum = phrase.strip().split()\n",
    "    encoded_datum = map(lambda x: dictionary[x] if x in dictionary else dictionary['UNK'], datum)\n",
    "    feed_data.append(encoded_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK\n"
     ]
    }
   ],
   "source": [
    "print reverse_dictionary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained predictions for: 1\n",
      "obtained predictions for: 65\n",
      "obtained predictions for: 129\n",
      "obtained predictions for: 193\n",
      "obtained predictions for: 257\n",
      "obtained predictions for: 321\n",
      "obtained predictions for: 385\n",
      "obtained predictions for: 449\n",
      "obtained predictions for: 513\n",
      "obtained predictions for: 577\n",
      "obtained predictions for: 641\n",
      "obtained predictions for: 705\n",
      "obtained predictions for: 769\n",
      "obtained predictions for: 833\n",
      "obtained predictions for: 897\n",
      "obtained predictions for: 961\n",
      "obtained predictions for: 1025\n",
      "obtained predictions for: 1089\n",
      "obtained predictions for: 1153\n",
      "obtained predictions for: 1217\n",
      "obtained predictions for: 1281\n",
      "obtained predictions for: 1345\n",
      "obtained predictions for: 1409\n",
      "obtained predictions for: 1473\n",
      "obtained predictions for: 1537\n",
      "obtained predictions for: 1601\n",
      "obtained predictions for: 1665\n",
      "obtained predictions for: 1729\n",
      "obtained predictions for: 1793\n",
      "obtained predictions for: 1857\n",
      "obtained predictions for: 1921\n",
      "obtained predictions for: 1985\n",
      "obtained predictions for: 2049\n",
      "obtained predictions for: 2113\n",
      "obtained predictions for: 2177\n",
      "obtained predictions for: 2241\n",
      "obtained predictions for: 2305\n",
      "obtained predictions for: 2369\n",
      "obtained predictions for: 2433\n",
      "obtained predictions for: 2497\n",
      "obtained predictions for: 2561\n",
      "obtained predictions for: 2625\n",
      "obtained predictions for: 2689\n",
      "obtained predictions for: 2753\n",
      "obtained predictions for: 2817\n",
      "obtained predictions for: 2881\n",
      "obtained predictions for: 2945\n",
      "obtained predictions for: 3009\n",
      "obtained predictions for: 3073\n",
      "obtained predictions for: 3137\n",
      "obtained predictions for: 3201\n",
      "obtained predictions for: 3265\n",
      "obtained predictions for: 3329\n",
      "obtained predictions for: 3393\n",
      "obtained predictions for: 3457\n",
      "obtained predictions for: 3521\n",
      "obtained predictions for: 3585\n",
      "obtained predictions for: 3649\n",
      "obtained predictions for: 3713\n",
      "obtained predictions for: 3777\n",
      "obtained predictions for: 3841\n",
      "obtained predictions for: 3905\n",
      "obtained predictions for: 3969\n",
      "obtained predictions for: 4033\n",
      "obtained predictions for: 4097\n",
      "obtained predictions for: 4161\n",
      "obtained predictions for: 4225\n",
      "obtained predictions for: 4289\n",
      "obtained predictions for: 4353\n",
      "obtained predictions for: 4417\n",
      "obtained predictions for: 4481\n",
      "obtained predictions for: 4545\n",
      "obtained predictions for: 4609\n",
      "obtained predictions for: 4673\n",
      "obtained predictions for: 4737\n",
      "obtained predictions for: 4801\n",
      "obtained predictions for: 4865\n",
      "obtained predictions for: 4929\n",
      "obtained predictions for: 4993\n",
      "obtained predictions for: 5057\n",
      "obtained predictions for: 5121\n",
      "obtained predictions for: 5185\n",
      "obtained predictions for: 5249\n",
      "obtained predictions for: 5313\n",
      "obtained predictions for: 5377\n",
      "obtained predictions for: 5441\n",
      "obtained predictions for: 5505\n",
      "obtained predictions for: 5569\n",
      "obtained predictions for: 5633\n",
      "obtained predictions for: 5697\n",
      "obtained predictions for: 5761\n",
      "obtained predictions for: 5825\n",
      "obtained predictions for: 5889\n",
      "obtained predictions for: 5953\n",
      "obtained predictions for: 6017\n",
      "obtained predictions for: 6081\n",
      "obtained predictions for: 6145\n",
      "obtained predictions for: 6209\n",
      "obtained predictions for: 6273\n",
      "obtained predictions for: 6337\n",
      "obtained predictions for: 6401\n",
      "obtained predictions for: 6465\n",
      "obtained predictions for: 6529\n",
      "obtained predictions for: 6593\n",
      "obtained predictions for: 6657\n",
      "obtained predictions for: 6721\n",
      "obtained predictions for: 6785\n",
      "obtained predictions for: 6849\n",
      "obtained predictions for: 6913\n",
      "obtained predictions for: 6977\n",
      "obtained predictions for: 7041\n",
      "obtained predictions for: 7105\n",
      "obtained predictions for: 7169\n",
      "obtained predictions for: 7233\n",
      "obtained predictions for: 7297\n",
      "obtained predictions for: 7361\n",
      "obtained predictions for: 7425\n",
      "obtained predictions for: 7489\n",
      "obtained predictions for: 7553\n",
      "obtained predictions for: 7617\n",
      "obtained predictions for: 7681\n",
      "obtained predictions for: 7745\n",
      "obtained predictions for: 7809\n",
      "obtained predictions for: 7873\n",
      "obtained predictions for: 7937\n",
      "obtained predictions for: 8001\n",
      "obtained predictions for: 8065\n",
      "obtained predictions for: 8129\n",
      "obtained predictions for: 8193\n",
      "obtained predictions for: 8257\n",
      "obtained predictions for: 8321\n",
      "obtained predictions for: 8385\n",
      "obtained predictions for: 8449\n",
      "obtained predictions for: 8513\n",
      "obtained predictions for: 8577\n",
      "obtained predictions for: 8641\n",
      "obtained predictions for: 8705\n",
      "obtained predictions for: 8769\n",
      "obtained predictions for: 8833\n",
      "obtained predictions for: 8897\n",
      "obtained predictions for: 8961\n",
      "obtained predictions for: 9025\n",
      "obtained predictions for: 9089\n",
      "obtained predictions for: 9153\n",
      "obtained predictions for: 9217\n",
      "obtained predictions for: 9281\n",
      "obtained predictions for: 9345\n",
      "obtained predictions for: 9409\n",
      "obtained predictions for: 9473\n",
      "obtained predictions for: 9537\n",
      "obtained predictions for: 9601\n",
      "obtained predictions for: 9665\n",
      "obtained predictions for: 9729\n",
      "obtained predictions for: 9793\n",
      "obtained predictions for: 9857\n",
      "obtained predictions for: 9921\n",
      "obtained predictions for: 9985\n",
      "obtained predictions for: 10049\n",
      "obtained predictions for: 10113\n",
      "obtained predictions for: 10177\n",
      "obtained predictions for: 10241\n",
      "obtained predictions for: 10305\n",
      "obtained predictions for: 10369\n",
      "obtained predictions for: 10433\n",
      "obtained predictions for: 10497\n",
      "obtained predictions for: 10561\n",
      "obtained predictions for: 10625\n",
      "obtained predictions for: 10689\n",
      "obtained predictions for: 10753\n",
      "obtained predictions for: 10817\n",
      "obtained predictions for: 10881\n",
      "obtained predictions for: 10945\n",
      "obtained predictions for: 11009\n",
      "obtained predictions for: 11073\n",
      "obtained predictions for: 11137\n",
      "obtained predictions for: 11201\n",
      "obtained predictions for: 11265\n",
      "obtained predictions for: 11329\n",
      "obtained predictions for: 11393\n",
      "obtained predictions for: 11457\n",
      "obtained predictions for: 11521\n",
      "obtained predictions for: 11585\n",
      "obtained predictions for: 11649\n",
      "obtained predictions for: 11713\n",
      "obtained predictions for: 11777\n",
      "obtained predictions for: 11841\n",
      "obtained predictions for: 11905\n",
      "obtained predictions for: 11969\n",
      "obtained predictions for: 12033\n",
      "obtained predictions for: 12097\n",
      "obtained predictions for: 12161\n",
      "obtained predictions for: 12225\n",
      "obtained predictions for: 12289\n",
      "obtained predictions for: 12353\n",
      "obtained predictions for: 12417\n",
      "obtained predictions for: 12481\n",
      "obtained predictions for: 12545\n",
      "obtained predictions for: 12609\n",
      "obtained predictions for: 12673\n",
      "obtained predictions for: 12737\n",
      "obtained predictions for: 12801\n",
      "obtained predictions for: 12865\n",
      "obtained predictions for: 12929\n",
      "obtained predictions for: 12993\n",
      "obtained predictions for: 13057\n",
      "obtained predictions for: 13121\n",
      "obtained predictions for: 13185\n",
      "obtained predictions for: 13249\n",
      "obtained predictions for: 13313\n",
      "obtained predictions for: 13377\n",
      "obtained predictions for: 13441\n",
      "obtained predictions for: 13505\n",
      "obtained predictions for: 13569\n",
      "obtained predictions for: 13633\n",
      "obtained predictions for: 13697\n",
      "obtained predictions for: 13761\n",
      "obtained predictions for: 13825\n",
      "obtained predictions for: 13889\n",
      "obtained predictions for: 13953\n",
      "obtained predictions for: 14017\n",
      "obtained predictions for: 14081\n",
      "obtained predictions for: 14145\n",
      "obtained predictions for: 14209\n",
      "obtained predictions for: 14273\n",
      "obtained predictions for: 14337\n",
      "obtained predictions for: 14401\n",
      "obtained predictions for: 14465\n",
      "obtained predictions for: 14529\n",
      "obtained predictions for: 14593\n",
      "obtained predictions for: 14657\n",
      "obtained predictions for: 14721\n",
      "obtained predictions for: 14785\n",
      "obtained predictions for: 14849\n",
      "obtained predictions for: 14913\n",
      "obtained predictions for: 14977\n",
      "obtained predictions for: 15041\n",
      "obtained predictions for: 15105\n",
      "obtained predictions for: 15169\n",
      "obtained predictions for: 15233\n",
      "obtained predictions for: 15297\n",
      "obtained predictions for: 15361\n",
      "obtained predictions for: 15425\n",
      "obtained predictions for: 15489\n",
      "obtained predictions for: 15553\n",
      "obtained predictions for: 15617\n",
      "obtained predictions for: 15681\n",
      "obtained predictions for: 15745\n",
      "obtained predictions for: 15809\n",
      "obtained predictions for: 15873\n",
      "obtained predictions for: 15937\n",
      "obtained predictions for: 16001\n",
      "obtained predictions for: 16065\n",
      "obtained predictions for: 16129\n",
      "obtained predictions for: 16193\n",
      "obtained predictions for: 16257\n",
      "obtained predictions for: 16321\n",
      "obtained predictions for: 16385\n",
      "obtained predictions for: 16449\n",
      "obtained predictions for: 16513\n",
      "obtained predictions for: 16577\n",
      "obtained predictions for: 16641\n",
      "obtained predictions for: 16705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained predictions for: 16769\n",
      "obtained predictions for: 16833\n",
      "obtained predictions for: 16897\n",
      "obtained predictions for: 16961\n",
      "obtained predictions for: 17025\n",
      "obtained predictions for: 17089\n",
      "obtained predictions for: 17153\n",
      "obtained predictions for: 17217\n",
      "obtained predictions for: 17281\n",
      "obtained predictions for: 17345\n",
      "obtained predictions for: 17409\n",
      "obtained predictions for: 17473\n",
      "obtained predictions for: 17537\n",
      "obtained predictions for: 17601\n",
      "obtained predictions for: 17665\n",
      "obtained predictions for: 17729\n",
      "obtained predictions for: 17793\n",
      "obtained predictions for: 17857\n",
      "obtained predictions for: 17921\n",
      "obtained predictions for: 17985\n",
      "obtained predictions for: 18049\n",
      "obtained predictions for: 18113\n",
      "obtained predictions for: 18177\n",
      "obtained predictions for: 18241\n",
      "obtained predictions for: 18305\n",
      "obtained predictions for: 18369\n",
      "obtained predictions for: 18433\n",
      "obtained predictions for: 18497\n",
      "obtained predictions for: 18561\n",
      "obtained predictions for: 18625\n",
      "obtained predictions for: 18689\n",
      "obtained predictions for: 18753\n",
      "obtained predictions for: 18817\n",
      "obtained predictions for: 18881\n",
      "obtained predictions for: 18945\n",
      "obtained predictions for: 19009\n",
      "obtained predictions for: 19073\n",
      "obtained predictions for: 19137\n",
      "obtained predictions for: 19201\n",
      "obtained predictions for: 19265\n",
      "obtained predictions for: 19329\n",
      "obtained predictions for: 19393\n",
      "obtained predictions for: 19457\n",
      "obtained predictions for: 19521\n",
      "obtained predictions for: 19585\n",
      "obtained predictions for: 19649\n",
      "obtained predictions for: 19713\n",
      "obtained predictions for: 19777\n",
      "obtained predictions for: 19841\n",
      "obtained predictions for: 19905\n",
      "obtained predictions for: 19969\n",
      "obtained predictions for: 20033\n",
      "obtained predictions for: 20097\n",
      "obtained predictions for: 20161\n",
      "obtained predictions for: 20225\n",
      "obtained predictions for: 20289\n",
      "obtained predictions for: 20353\n",
      "obtained predictions for: 20417\n",
      "obtained predictions for: 20481\n",
      "obtained predictions for: 20545\n",
      "obtained predictions for: 20609\n",
      "obtained predictions for: 20673\n",
      "obtained predictions for: 20737\n",
      "obtained predictions for: 20801\n",
      "obtained predictions for: 20865\n",
      "obtained predictions for: 20929\n",
      "obtained predictions for: 20993\n",
      "obtained predictions for: 21057\n",
      "obtained predictions for: 21121\n",
      "obtained predictions for: 21185\n",
      "obtained predictions for: 21249\n",
      "obtained predictions for: 21313\n",
      "obtained predictions for: 21377\n",
      "obtained predictions for: 21441\n",
      "obtained predictions for: 21505\n",
      "obtained predictions for: 21569\n",
      "obtained predictions for: 21633\n",
      "obtained predictions for: 21697\n",
      "obtained predictions for: 21761\n",
      "obtained predictions for: 21825\n",
      "obtained predictions for: 21889\n",
      "obtained predictions for: 21953\n",
      "obtained predictions for: 22017\n",
      "obtained predictions for: 22081\n",
      "obtained predictions for: 22145\n",
      "obtained predictions for: 22209\n",
      "obtained predictions for: 22273\n",
      "obtained predictions for: 22337\n",
      "obtained predictions for: 22401\n",
      "obtained predictions for: 22465\n",
      "obtained predictions for: 22529\n",
      "obtained predictions for: 22593\n",
      "obtained predictions for: 22657\n",
      "obtained predictions for: 22721\n",
      "obtained predictions for: 22785\n",
      "obtained predictions for: 22849\n",
      "obtained predictions for: 22913\n",
      "obtained predictions for: 22977\n",
      "obtained predictions for: 23041\n",
      "obtained predictions for: 23105\n",
      "obtained predictions for: 23169\n",
      "obtained predictions for: 23233\n",
      "obtained predictions for: 23297\n",
      "obtained predictions for: 23361\n",
      "obtained predictions for: 23425\n",
      "obtained predictions for: 23489\n",
      "obtained predictions for: 23553\n",
      "obtained predictions for: 23617\n",
      "obtained predictions for: 23681\n",
      "obtained predictions for: 23745\n",
      "obtained predictions for: 23809\n",
      "obtained predictions for: 23873\n",
      "obtained predictions for: 23937\n",
      "obtained predictions for: 24001\n",
      "obtained predictions for: 24065\n",
      "obtained predictions for: 24129\n",
      "obtained predictions for: 24193\n",
      "obtained predictions for: 24257\n",
      "obtained predictions for: 24321\n",
      "obtained predictions for: 24385\n",
      "obtained predictions for: 24449\n",
      "obtained predictions for: 24513\n",
      "obtained predictions for: 24577\n",
      "obtained predictions for: 24641\n",
      "obtained predictions for: 24705\n",
      "obtained predictions for: 24769\n",
      "obtained predictions for: 24833\n",
      "obtained predictions for: 24897\n",
      "obtained predictions for: 24961\n",
      "obtained predictions for: 25025\n",
      "obtained predictions for: 25089\n",
      "obtained predictions for: 25153\n",
      "obtained predictions for: 25217\n",
      "obtained predictions for: 25281\n",
      "obtained predictions for: 25345\n",
      "obtained predictions for: 25409\n",
      "obtained predictions for: 25473\n",
      "obtained predictions for: 25537\n",
      "obtained predictions for: 25601\n",
      "obtained predictions for: 25665\n",
      "obtained predictions for: 25729\n",
      "obtained predictions for: 25793\n",
      "obtained predictions for: 25857\n",
      "obtained predictions for: 25921\n",
      "obtained predictions for: 25985\n",
      "obtained predictions for: 26049\n",
      "obtained predictions for: 26113\n",
      "obtained predictions for: 26177\n",
      "obtained predictions for: 26241\n",
      "obtained predictions for: 26305\n",
      "obtained predictions for: 26369\n",
      "obtained predictions for: 26433\n",
      "obtained predictions for: 26497\n",
      "obtained predictions for: 26561\n",
      "obtained predictions for: 26625\n",
      "obtained predictions for: 26689\n",
      "obtained predictions for: 26753\n",
      "obtained predictions for: 26817\n",
      "obtained predictions for: 26881\n",
      "obtained predictions for: 26945\n",
      "obtained predictions for: 27009\n",
      "obtained predictions for: 27073\n",
      "obtained predictions for: 27137\n",
      "obtained predictions for: 27201\n",
      "obtained predictions for: 27265\n",
      "obtained predictions for: 27329\n",
      "obtained predictions for: 27393\n",
      "obtained predictions for: 27457\n",
      "obtained predictions for: 27521\n",
      "obtained predictions for: 27585\n",
      "obtained predictions for: 27649\n",
      "obtained predictions for: 27713\n",
      "obtained predictions for: 27777\n",
      "obtained predictions for: 27841\n",
      "obtained predictions for: 27905\n",
      "obtained predictions for: 27969\n",
      "obtained predictions for: 28033\n",
      "obtained predictions for: 28097\n",
      "obtained predictions for: 28161\n",
      "obtained predictions for: 28225\n",
      "obtained predictions for: 28289\n",
      "obtained predictions for: 28353\n",
      "obtained predictions for: 28417\n",
      "obtained predictions for: 28481\n",
      "obtained predictions for: 28545\n",
      "obtained predictions for: 28609\n",
      "obtained predictions for: 28673\n",
      "obtained predictions for: 28737\n",
      "obtained predictions for: 28801\n",
      "obtained predictions for: 28865\n",
      "obtained predictions for: 28929\n",
      "obtained predictions for: 28993\n",
      "obtained predictions for: 29057\n",
      "obtained predictions for: 29121\n",
      "obtained predictions for: 29185\n",
      "obtained predictions for: 29249\n",
      "obtained predictions for: 29313\n",
      "obtained predictions for: 29377\n",
      "obtained predictions for: 29441\n",
      "obtained predictions for: 29505\n",
      "obtained predictions for: 29569\n",
      "obtained predictions for: 29633\n",
      "obtained predictions for: 29697\n",
      "obtained predictions for: 29761\n",
      "obtained predictions for: 29825\n",
      "obtained predictions for: 29889\n",
      "obtained predictions for: 29953\n",
      "obtained predictions for: 30017\n",
      "obtained predictions for: 30081\n",
      "obtained predictions for: 30145\n",
      "obtained predictions for: 30209\n",
      "obtained predictions for: 30273\n",
      "obtained predictions for: 30337\n",
      "obtained predictions for: 30401\n",
      "obtained predictions for: 30465\n",
      "obtained predictions for: 30529\n",
      "obtained predictions for: 30593\n",
      "obtained predictions for: 30657\n",
      "obtained predictions for: 30721\n",
      "obtained predictions for: 30785\n",
      "obtained predictions for: 30849\n",
      "obtained predictions for: 30913\n",
      "obtained predictions for: 30977\n",
      "obtained predictions for: 31041\n",
      "obtained predictions for: 31105\n",
      "obtained predictions for: 31169\n",
      "obtained predictions for: 31233\n",
      "obtained predictions for: 31297\n",
      "obtained predictions for: 31361\n",
      "obtained predictions for: 31425\n",
      "obtained predictions for: 31489\n",
      "obtained predictions for: 31553\n",
      "obtained predictions for: 31617\n",
      "obtained predictions for: 31681\n",
      "obtained predictions for: 31745\n",
      "obtained predictions for: 31809\n",
      "obtained predictions for: 31873\n",
      "obtained predictions for: 31937\n",
      "obtained predictions for: 32001\n",
      "obtained predictions for: 32065\n",
      "obtained predictions for: 32129\n",
      "obtained predictions for: 32193\n",
      "obtained predictions for: 32257\n",
      "obtained predictions for: 32321\n",
      "obtained predictions for: 32385\n",
      "obtained predictions for: 32449\n",
      "obtained predictions for: 32513\n",
      "obtained predictions for: 32577\n",
      "obtained predictions for: 32641\n",
      "obtained predictions for: 32705\n",
      "obtained predictions for: 32769\n",
      "obtained predictions for: 32833\n",
      "obtained predictions for: 32897\n",
      "obtained predictions for: 32961\n",
      "obtained predictions for: 33025\n",
      "obtained predictions for: 33089\n",
      "obtained predictions for: 33153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained predictions for: 33217\n",
      "obtained predictions for: 33281\n",
      "obtained predictions for: 33345\n",
      "obtained predictions for: 33409\n",
      "obtained predictions for: 33473\n",
      "obtained predictions for: 33537\n",
      "obtained predictions for: 33601\n",
      "obtained predictions for: 33665\n",
      "obtained predictions for: 33729\n",
      "obtained predictions for: 33793\n",
      "obtained predictions for: 33857\n",
      "obtained predictions for: 33921\n",
      "obtained predictions for: 33985\n",
      "obtained predictions for: 34049\n",
      "obtained predictions for: 34113\n",
      "obtained predictions for: 34177\n",
      "obtained predictions for: 34241\n",
      "obtained predictions for: 34305\n",
      "obtained predictions for: 34369\n",
      "obtained predictions for: 34433\n",
      "obtained predictions for: 34497\n",
      "obtained predictions for: 34561\n",
      "obtained predictions for: 34625\n",
      "obtained predictions for: 34689\n",
      "obtained predictions for: 34753\n",
      "obtained predictions for: 34817\n",
      "obtained predictions for: 34881\n",
      "obtained predictions for: 34945\n",
      "obtained predictions for: 35009\n",
      "obtained predictions for: 35073\n",
      "obtained predictions for: 35137\n",
      "obtained predictions for: 35201\n",
      "obtained predictions for: 35265\n",
      "obtained predictions for: 35329\n",
      "obtained predictions for: 35393\n",
      "obtained predictions for: 35457\n",
      "obtained predictions for: 35521\n",
      "obtained predictions for: 35585\n",
      "obtained predictions for: 35649\n",
      "obtained predictions for: 35713\n",
      "obtained predictions for: 35777\n",
      "obtained predictions for: 35841\n",
      "obtained predictions for: 35905\n",
      "obtained predictions for: 35969\n",
      "obtained predictions for: 36033\n",
      "obtained predictions for: 36097\n",
      "obtained predictions for: 36161\n",
      "obtained predictions for: 36225\n",
      "obtained predictions for: 36289\n",
      "obtained predictions for: 36353\n",
      "obtained predictions for: 36417\n",
      "obtained predictions for: 36481\n",
      "obtained predictions for: 36545\n",
      "obtained predictions for: 36609\n",
      "obtained predictions for: 36673\n",
      "obtained predictions for: 36737\n",
      "obtained predictions for: 36801\n",
      "obtained predictions for: 36865\n",
      "obtained predictions for: 36929\n",
      "obtained predictions for: 36993\n",
      "obtained predictions for: 37057\n",
      "obtained predictions for: 37121\n",
      "obtained predictions for: 37185\n",
      "obtained predictions for: 37249\n",
      "obtained predictions for: 37313\n",
      "obtained predictions for: 37377\n",
      "obtained predictions for: 37441\n",
      "obtained predictions for: 37505\n",
      "obtained predictions for: 37569\n",
      "obtained predictions for: 37633\n",
      "obtained predictions for: 37697\n",
      "obtained predictions for: 37761\n",
      "obtained predictions for: 37825\n",
      "obtained predictions for: 37889\n",
      "obtained predictions for: 37953\n",
      "obtained predictions for: 38017\n",
      "obtained predictions for: 38081\n",
      "obtained predictions for: 38145\n",
      "obtained predictions for: 38209\n",
      "obtained predictions for: 38273\n",
      "obtained predictions for: 38337\n",
      "obtained predictions for: 38401\n",
      "obtained predictions for: 38465\n",
      "obtained predictions for: 38529\n",
      "obtained predictions for: 38593\n",
      "obtained predictions for: 38657\n",
      "obtained predictions for: 38721\n",
      "obtained predictions for: 38785\n",
      "obtained predictions for: 38849\n",
      "obtained predictions for: 38913\n",
      "obtained predictions for: 38977\n",
      "obtained predictions for: 39041\n",
      "obtained predictions for: 39105\n",
      "obtained predictions for: 39169\n",
      "obtained predictions for: 39233\n",
      "obtained predictions for: 39297\n",
      "obtained predictions for: 39361\n",
      "obtained predictions for: 39425\n",
      "obtained predictions for: 39489\n",
      "obtained predictions for: 39553\n",
      "obtained predictions for: 39617\n",
      "obtained predictions for: 39681\n",
      "obtained predictions for: 39745\n",
      "obtained predictions for: 39809\n",
      "obtained predictions for: 39873\n",
      "obtained predictions for: 39937\n",
      "obtained predictions for: 40001\n",
      "obtained predictions for: 40065\n",
      "obtained predictions for: 40129\n",
      "obtained predictions for: 40193\n",
      "obtained predictions for: 40257\n",
      "obtained predictions for: 40321\n",
      "obtained predictions for: 40385\n",
      "obtained predictions for: 40449\n",
      "obtained predictions for: 40513\n",
      "obtained predictions for: 40577\n",
      "obtained predictions for: 40641\n",
      "obtained predictions for: 40705\n",
      "obtained predictions for: 40769\n",
      "obtained predictions for: 40833\n",
      "obtained predictions for: 40897\n",
      "obtained predictions for: 40961\n",
      "obtained predictions for: 41025\n",
      "obtained predictions for: 41089\n",
      "obtained predictions for: 41153\n",
      "obtained predictions for: 41217\n",
      "obtained predictions for: 41281\n",
      "obtained predictions for: 41345\n",
      "obtained predictions for: 41409\n",
      "obtained predictions for: 41473\n",
      "obtained predictions for: 41537\n",
      "obtained predictions for: 41601\n",
      "obtained predictions for: 41665\n",
      "obtained predictions for: 41729\n",
      "obtained predictions for: 41793\n",
      "obtained predictions for: 41857\n",
      "obtained predictions for: 41921\n",
      "obtained predictions for: 41985\n",
      "obtained predictions for: 42049\n",
      "obtained predictions for: 42113\n",
      "obtained predictions for: 42177\n",
      "obtained predictions for: 42241\n",
      "obtained predictions for: 42305\n",
      "obtained predictions for: 42369\n",
      "obtained predictions for: 42433\n",
      "obtained predictions for: 42497\n",
      "obtained predictions for: 42561\n",
      "obtained predictions for: 42625\n",
      "obtained predictions for: 42689\n",
      "obtained predictions for: 42753\n",
      "obtained predictions for: 42817\n",
      "obtained predictions for: 42881\n",
      "obtained predictions for: 42945\n",
      "obtained predictions for: 43009\n",
      "obtained predictions for: 43073\n",
      "obtained predictions for: 43137\n",
      "obtained predictions for: 43201\n",
      "obtained predictions for: 43265\n",
      "obtained predictions for: 43329\n",
      "obtained predictions for: 43393\n",
      "obtained predictions for: 43457\n",
      "obtained predictions for: 43521\n",
      "obtained predictions for: 43585\n",
      "obtained predictions for: 43649\n",
      "obtained predictions for: 43713\n",
      "obtained predictions for: 43777\n",
      "obtained predictions for: 43841\n",
      "obtained predictions for: 43905\n",
      "obtained predictions for: 43969\n",
      "obtained predictions for: 44033\n",
      "obtained predictions for: 44097\n",
      "obtained predictions for: 44161\n",
      "obtained predictions for: 44225\n",
      "obtained predictions for: 44289\n",
      "obtained predictions for: 44353\n",
      "obtained predictions for: 44417\n",
      "obtained predictions for: 44481\n",
      "obtained predictions for: 44545\n",
      "obtained predictions for: 44609\n",
      "obtained predictions for: 44673\n",
      "obtained predictions for: 44737\n",
      "obtained predictions for: 44801\n",
      "obtained predictions for: 44865\n",
      "obtained predictions for: 44929\n",
      "obtained predictions for: 44993\n",
      "obtained predictions for: 45057\n",
      "obtained predictions for: 45121\n",
      "obtained predictions for: 45185\n",
      "obtained predictions for: 45249\n",
      "obtained predictions for: 45313\n",
      "obtained predictions for: 45377\n",
      "obtained predictions for: 45441\n",
      "obtained predictions for: 45505\n",
      "obtained predictions for: 45569\n",
      "obtained predictions for: 45633\n",
      "obtained predictions for: 45697\n",
      "obtained predictions for: 45761\n",
      "obtained predictions for: 45825\n",
      "obtained predictions for: 45889\n",
      "obtained predictions for: 45953\n",
      "obtained predictions for: 46017\n",
      "obtained predictions for: 46081\n",
      "obtained predictions for: 46145\n",
      "obtained predictions for: 46209\n",
      "obtained predictions for: 46273\n",
      "obtained predictions for: 46337\n",
      "obtained predictions for: 46401\n",
      "obtained predictions for: 46465\n",
      "obtained predictions for: 46529\n",
      "obtained predictions for: 46593\n",
      "obtained predictions for: 46657\n",
      "obtained predictions for: 46721\n",
      "obtained predictions for: 46785\n",
      "obtained predictions for: 46849\n",
      "obtained predictions for: 46913\n",
      "obtained predictions for: 46977\n",
      "obtained predictions for: 47041\n",
      "obtained predictions for: 47105\n",
      "obtained predictions for: 47169\n",
      "obtained predictions for: 47233\n",
      "obtained predictions for: 47297\n",
      "obtained predictions for: 47361\n",
      "obtained predictions for: 47425\n",
      "obtained predictions for: 47489\n",
      "obtained predictions for: 47553\n",
      "obtained predictions for: 47617\n",
      "obtained predictions for: 47681\n",
      "obtained predictions for: 47745\n",
      "obtained predictions for: 47809\n",
      "obtained predictions for: 47873\n",
      "obtained predictions for: 47937\n",
      "obtained predictions for: 48001\n",
      "obtained predictions for: 48065\n",
      "obtained predictions for: 48129\n",
      "obtained predictions for: 48193\n",
      "obtained predictions for: 48257\n",
      "obtained predictions for: 48321\n",
      "obtained predictions for: 48385\n",
      "obtained predictions for: 48449\n",
      "obtained predictions for: 48513\n",
      "obtained predictions for: 48577\n",
      "obtained predictions for: 48641\n",
      "obtained predictions for: 48705\n",
      "obtained predictions for: 48769\n",
      "obtained predictions for: 48833\n",
      "obtained predictions for: 48897\n",
      "obtained predictions for: 48961\n",
      "obtained predictions for: 49025\n",
      "obtained predictions for: 49089\n",
      "obtained predictions for: 49153\n",
      "obtained predictions for: 49217\n",
      "obtained predictions for: 49281\n",
      "obtained predictions for: 49345\n",
      "obtained predictions for: 49409\n",
      "obtained predictions for: 49473\n",
      "obtained predictions for: 49537\n",
      "obtained predictions for: 49601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained predictions for: 49665\n",
      "obtained predictions for: 49729\n",
      "obtained predictions for: 49793\n",
      "obtained predictions for: 49857\n",
      "obtained predictions for: 49921\n",
      "obtained predictions for: 49985\n",
      "obtained predictions for: 50049\n",
      "obtained predictions for: 50113\n",
      "obtained predictions for: 50177\n",
      "obtained predictions for: 50241\n",
      "obtained predictions for: 50305\n",
      "obtained predictions for: 50369\n",
      "obtained predictions for: 50433\n",
      "obtained predictions for: 50497\n",
      "obtained predictions for: 50561\n",
      "obtained predictions for: 50625\n",
      "obtained predictions for: 50689\n",
      "obtained predictions for: 50753\n",
      "obtained predictions for: 50817\n",
      "obtained predictions for: 50881\n",
      "obtained predictions for: 50945\n",
      "obtained predictions for: 51009\n",
      "obtained predictions for: 51073\n",
      "obtained predictions for: 51137\n",
      "obtained predictions for: 51201\n",
      "obtained predictions for: 51265\n",
      "obtained predictions for: 51329\n",
      "obtained predictions for: 51393\n",
      "obtained predictions for: 51457\n",
      "obtained predictions for: 51521\n",
      "obtained predictions for: 51585\n",
      "obtained predictions for: 51649\n",
      "obtained predictions for: 51713\n",
      "obtained predictions for: 51777\n",
      "obtained predictions for: 51841\n",
      "obtained predictions for: 51905\n",
      "obtained predictions for: 51969\n",
      "obtained predictions for: 52033\n",
      "obtained predictions for: 52097\n",
      "obtained predictions for: 52161\n",
      "obtained predictions for: 52225\n",
      "obtained predictions for: 52289\n",
      "obtained predictions for: 52353\n",
      "obtained predictions for: 52417\n",
      "obtained predictions for: 52481\n",
      "obtained predictions for: 52545\n",
      "obtained predictions for: 52609\n",
      "obtained predictions for: 52673\n",
      "obtained predictions for: 52737\n",
      "obtained predictions for: 52801\n",
      "obtained predictions for: 52865\n",
      "obtained predictions for: 52929\n",
      "obtained predictions for: 52993\n",
      "obtained predictions for: 53057\n",
      "obtained predictions for: 53121\n",
      "obtained predictions for: 53185\n",
      "obtained predictions for: 53249\n",
      "obtained predictions for: 53313\n",
      "obtained predictions for: 53377\n",
      "obtained predictions for: 53441\n",
      "obtained predictions for: 53505\n",
      "obtained predictions for: 53569\n",
      "obtained predictions for: 53633\n",
      "obtained predictions for: 53697\n",
      "obtained predictions for: 53761\n",
      "obtained predictions for: 53825\n",
      "obtained predictions for: 53889\n",
      "obtained predictions for: 53953\n",
      "obtained predictions for: 54017\n",
      "obtained predictions for: 54081\n",
      "obtained predictions for: 54145\n",
      "obtained predictions for: 54209\n",
      "obtained predictions for: 54273\n",
      "obtained predictions for: 54337\n",
      "obtained predictions for: 54401\n",
      "obtained predictions for: 54465\n",
      "obtained predictions for: 54529\n",
      "obtained predictions for: 54593\n",
      "obtained predictions for: 54657\n",
      "obtained predictions for: 54721\n",
      "obtained predictions for: 54785\n",
      "obtained predictions for: 54849\n",
      "obtained predictions for: 54913\n",
      "obtained predictions for: 54977\n",
      "obtained predictions for: 55041\n",
      "obtained predictions for: 55105\n",
      "obtained predictions for: 55169\n",
      "obtained predictions for: 55233\n",
      "obtained predictions for: 55297\n",
      "obtained predictions for: 55361\n",
      "obtained predictions for: 55425\n",
      "obtained predictions for: 55489\n",
      "obtained predictions for: 55553\n",
      "obtained predictions for: 55617\n",
      "obtained predictions for: 55681\n",
      "obtained predictions for: 55745\n",
      "obtained predictions for: 55809\n",
      "obtained predictions for: 55873\n",
      "obtained predictions for: 55937\n",
      "obtained predictions for: 56001\n",
      "obtained predictions for: 56065\n",
      "obtained predictions for: 56129\n",
      "obtained predictions for: 56193\n",
      "obtained predictions for: 56257\n",
      "obtained predictions for: 56321\n",
      "obtained predictions for: 56385\n",
      "obtained predictions for: 56449\n",
      "obtained predictions for: 56513\n",
      "obtained predictions for: 56577\n",
      "obtained predictions for: 56641\n",
      "obtained predictions for: 56705\n",
      "obtained predictions for: 56769\n",
      "obtained predictions for: 56833\n",
      "obtained predictions for: 56897\n",
      "obtained predictions for: 56961\n",
      "obtained predictions for: 57025\n",
      "obtained predictions for: 57089\n",
      "obtained predictions for: 57153\n",
      "obtained predictions for: 57217\n",
      "obtained predictions for: 57281\n",
      "obtained predictions for: 57345\n",
      "obtained predictions for: 57409\n",
      "obtained predictions for: 57473\n",
      "obtained predictions for: 57537\n",
      "obtained predictions for: 57601\n",
      "obtained predictions for: 57665\n",
      "obtained predictions for: 57729\n",
      "obtained predictions for: 57793\n",
      "obtained predictions for: 57857\n",
      "obtained predictions for: 57921\n",
      "obtained predictions for: 57985\n",
      "obtained predictions for: 58049\n",
      "obtained predictions for: 58113\n",
      "obtained predictions for: 58177\n",
      "obtained predictions for: 58241\n",
      "obtained predictions for: 58305\n",
      "obtained predictions for: 58369\n",
      "obtained predictions for: 58433\n",
      "obtained predictions for: 58497\n",
      "obtained predictions for: 58561\n",
      "obtained predictions for: 58625\n",
      "obtained predictions for: 58689\n",
      "obtained predictions for: 58753\n",
      "obtained predictions for: 58817\n",
      "obtained predictions for: 58881\n",
      "obtained predictions for: 58945\n",
      "obtained predictions for: 59009\n",
      "obtained predictions for: 59073\n",
      "obtained predictions for: 59137\n",
      "obtained predictions for: 59201\n",
      "obtained predictions for: 59265\n",
      "obtained predictions for: 59329\n",
      "obtained predictions for: 59393\n",
      "obtained predictions for: 59457\n",
      "obtained predictions for: 59521\n",
      "obtained predictions for: 59585\n",
      "obtained predictions for: 59649\n",
      "obtained predictions for: 59713\n",
      "obtained predictions for: 59777\n",
      "obtained predictions for: 59841\n",
      "obtained predictions for: 59905\n",
      "obtained predictions for: 59969\n",
      "obtained predictions for: 60033\n",
      "obtained predictions for: 60097\n",
      "obtained predictions for: 60161\n",
      "obtained predictions for: 60225\n",
      "obtained predictions for: 60289\n",
      "obtained predictions for: 60353\n",
      "obtained predictions for: 60417\n",
      "obtained predictions for: 60481\n",
      "obtained predictions for: 60545\n",
      "obtained predictions for: 60609\n",
      "obtained predictions for: 60673\n",
      "obtained predictions for: 60737\n",
      "obtained predictions for: 60801\n",
      "obtained predictions for: 60865\n",
      "obtained predictions for: 60929\n",
      "obtained predictions for: 60993\n",
      "obtained predictions for: 61057\n",
      "obtained predictions for: 61121\n",
      "obtained predictions for: 61185\n",
      "obtained predictions for: 61249\n",
      "obtained predictions for: 61313\n",
      "obtained predictions for: 61377\n",
      "obtained predictions for: 61441\n",
      "obtained predictions for: 61505\n",
      "obtained predictions for: 61569\n",
      "obtained predictions for: 61633\n",
      "obtained predictions for: 61697\n",
      "obtained predictions for: 61761\n",
      "obtained predictions for: 61825\n",
      "obtained predictions for: 61889\n",
      "obtained predictions for: 61953\n",
      "obtained predictions for: 62017\n",
      "obtained predictions for: 62081\n",
      "obtained predictions for: 62145\n",
      "obtained predictions for: 62209\n",
      "obtained predictions for: 62273\n",
      "obtained predictions for: 62337\n",
      "obtained predictions for: 62401\n",
      "obtained predictions for: 62465\n",
      "obtained predictions for: 62529\n",
      "obtained predictions for: 62593\n",
      "obtained predictions for: 62657\n",
      "obtained predictions for: 62721\n",
      "obtained predictions for: 62785\n",
      "obtained predictions for: 62849\n",
      "obtained predictions for: 62913\n",
      "obtained predictions for: 62977\n",
      "obtained predictions for: 63041\n",
      "obtained predictions for: 63105\n",
      "obtained predictions for: 63169\n",
      "obtained predictions for: 63233\n",
      "obtained predictions for: 63297\n",
      "obtained predictions for: 63361\n",
      "obtained predictions for: 63425\n",
      "obtained predictions for: 63489\n",
      "obtained predictions for: 63553\n",
      "obtained predictions for: 63617\n",
      "obtained predictions for: 63681\n",
      "obtained predictions for: 63745\n",
      "obtained predictions for: 63809\n",
      "obtained predictions for: 63873\n",
      "obtained predictions for: 63937\n",
      "obtained predictions for: 64001\n",
      "obtained predictions for: 64065\n",
      "obtained predictions for: 64129\n",
      "obtained predictions for: 64193\n",
      "obtained predictions for: 64257\n",
      "obtained predictions for: 64321\n",
      "obtained predictions for: 64385\n",
      "obtained predictions for: 64449\n",
      "obtained predictions for: 64513\n",
      "obtained predictions for: 64577\n",
      "obtained predictions for: 64641\n",
      "obtained predictions for: 64705\n",
      "obtained predictions for: 64769\n",
      "obtained predictions for: 64833\n",
      "obtained predictions for: 64897\n",
      "obtained predictions for: 64961\n",
      "obtained predictions for: 65025\n",
      "obtained predictions for: 65089\n",
      "obtained predictions for: 65153\n",
      "obtained predictions for: 65217\n",
      "obtained predictions for: 65281\n",
      "obtained predictions for: 65345\n",
      "obtained predictions for: 65409\n",
      "obtained predictions for: 65473\n",
      "obtained predictions for: 65537\n",
      "obtained predictions for: 65601\n",
      "obtained predictions for: 65665\n",
      "obtained predictions for: 65729\n",
      "obtained predictions for: 65793\n",
      "obtained predictions for: 65857\n",
      "obtained predictions for: 65921\n",
      "obtained predictions for: 65985\n",
      "obtained predictions for: 66049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained predictions for: 66113\n",
      "obtained predictions for: 66177\n",
      "obtained predictions for: 66241\n"
     ]
    }
   ],
   "source": [
    "# run the loop to get the predictions\n",
    "index = 0; final_preds = [] # start with an empty list\n",
    "while(index < len(feed_data)):\n",
    "    start = index; end = start + batch_size\n",
    "    batch = feed_data[start: end]\n",
    "    small_preds = sess.run(predictions, feed_dict={tf_input_seqs: pad(batch)})\n",
    "    print \"obtained predictions for: \" + str(index + 1)\n",
    "    final_preds.append(small_preds)\n",
    "    index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "don_preds = np.concatenate(final_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "don_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamp_preds = np.argmax(don_preds, axis=1)\n",
    "tamp_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE HAS BEEN SAVED\n"
     ]
    }
   ],
   "source": [
    "# create a file and write this data to it:\n",
    "with open(submission_file, 'w') as sub:\n",
    "    # write the header to the file:\n",
    "    sub.write(\"PhraseId,Sentiment\\n\")\n",
    "    for (pid, predic) in zip(ids, tamp_preds):\n",
    "        line = str(pid) + \",\" + str(predic) + \"\\n\"\n",
    "        sub.write(line)\n",
    "print \"FILE HAS BEEN SAVED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's submit this file and see what happens!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
